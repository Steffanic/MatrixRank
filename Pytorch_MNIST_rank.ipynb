{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W15yOca4P99l"
      },
      "source": [
        "I want to study the relationship between the rank of the weight matrices of each layer and the problem at hand. I have a hunch that the convergent rank of the entire network(to be defined) will match the computational complexity of the task defined and implied by the loss function and the data. What I would love to have is equivalence classes of networks whose convergent ranks under SGD with weight decay are equal. I would also like to know whether or not shrinking or expanding the layers based on the convergent rank of a previous iteration of the network will yeild similar accuracy.\n",
        "\n",
        "Problem 1) Train N networks with randomly chosen networks with sufficient capacity for the problem. Train them under SGD with weight decay until the loss saturates. Compute the rank of the network(to be defined in a way that is comparable accross architechtures) and compare, look for trends. A quick expectation is that there will be cases of overfitting where the rank is higher than networks that perform better with lower rank. Essentially indicating that there is enough computational power to memorize the dataset.\n",
        "\n",
        "Problem 2) Develop an iterative algorithm that trains a large network, then prunes it according the per-layer rank. Retrain and reprune. Study the behaviour of the final accuracy and its relationship to trianing time. Another variation could be reducing the weight decay each iteration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bGU6NwlsXFSt"
      },
      "outputs": [],
      "source": [
        "#@title Import Dependencies\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_bNfVLRUYqZA"
      },
      "outputs": [],
      "source": [
        "#@title Define Hyperparameters\n",
        "\n",
        "input_size = 784 # img_size = (28,28) ---> 28*28=784 in total\n",
        "hidden_size_base = 784 # number of nodes at hidden layer\n",
        "num_classes = 10 # number of output classes discrete range [0,9]\n",
        "num_epochs = 100 # number of times which the entire dataset is passed throughout the model\n",
        "small_batch_size = 64 # the size of input data took for one iteration\n",
        "large_batch_size = 128 # the size of input data took for one iteration\n",
        "lr = 1e-3 # size of step\n",
        "model_layer_configs = [2,3,4,5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCsBCXMwbpH5",
        "outputId": "49518cb6-cbf0-436f-d49d-ab999085e47e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 16308709.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 75568118.42it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 24760097.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 14929881.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#@title Downloading MNIST data\n",
        "\n",
        "train_data = dsets.MNIST(root = './data', train = True,\n",
        "                        transform = transforms.ToTensor(), download = True)\n",
        "\n",
        "test_data = dsets.MNIST(root = './data', train = False,\n",
        "                       transform = transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "rfDPBdnYgfGp"
      },
      "outputs": [],
      "source": [
        "#@title Loading the data\n",
        "\n",
        "train_gen_small_batch = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = small_batch_size,\n",
        "                                             shuffle = True,\n",
        "                                             num_workers=0)\n",
        "\n",
        "test_gen_small_batch = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = small_batch_size,\n",
        "                                      shuffle = False,\n",
        "                                      num_workers=0)\n",
        "\n",
        "train_gen_large_batch = torch.utils.data.DataLoader(dataset = train_data,\n",
        "                                             batch_size = large_batch_size,\n",
        "                                             shuffle = True,\n",
        "                                             num_workers=0)\n",
        "\n",
        "test_gen_large_batch = torch.utils.data.DataLoader(dataset = test_data,\n",
        "                                      batch_size = large_batch_size,\n",
        "                                      shuffle = False,\n",
        "                                      num_workers=0)\n",
        "# train_data.data.to(\"cuda:0\")\n",
        "# train_data.targets.to(\"cuda:0\")\n",
        "# test_data.data.to(\"cuda:0\")\n",
        "# test_data.targets.to(\"cuda:0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "fL-YXTvghaz_"
      },
      "outputs": [],
      "source": [
        "#@title Define model class\n",
        "\n",
        "class Net(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, n_layers, num_classes, with_skip):\n",
        "    super(Net,self).__init__()\n",
        "    self.fc_layers = nn.ModuleList()\n",
        "    self.fc_layers.append(nn.Linear(input_size, hidden_size[0]))\n",
        "    for i in range(n_layers):\n",
        "      self.fc_layers.append(nn.Linear(hidden_size[i], hidden_size[i+1]))\n",
        "    self.fc_layers.append(nn.Linear(hidden_size[-1], num_classes))\n",
        "    self.relu = nn.ReLU()\n",
        "    self.n_layers = n_layers\n",
        "    self.with_skip = with_skip\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    in_val = x\n",
        "    for i,layer in enumerate(self.fc_layers):\n",
        "      out_val = layer(in_val)\n",
        "      if i!=self.n_layers+1:\n",
        "        out = self.relu(out_val)\n",
        "        in_val=out\n",
        "\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "5o0NoNT6GmGi"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "loss_function = nn.CrossEntropyLoss().cuda()\n",
        "optimizers = {}\n",
        "lr_scheds = {}\n",
        "optimized_ranks = {}\n",
        "history = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "-3EPEqbjjfAT"
      },
      "outputs": [],
      "source": [
        "for n_layers in model_layer_configs:\n",
        "  optimized_ranks[str(n_layers)] = [hidden_size_base for _ in range(int(n_layers)+1)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ePLIwvAFj2zH"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "u75Xa5VckuTH",
        "outputId": "cac30313-1f28-4837-db34-9c6ddb9510d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "n_layers:2, n_neurons: 3146\n",
            "n_layers:3, n_neurons: 3930\n",
            "n_layers:4, n_neurons: 4714\n",
            "n_layers:5, n_neurons: 5498\n",
            "Epoch [1/100], Step [500/937], Loss: {'2': tensor(1.7919, grad_fn=<NllLossBackward0>), '3': tensor(0.6880, grad_fn=<NllLossBackward0>), '4': tensor(3.2156, grad_fn=<NllLossBackward0>), '5': tensor(0.2288, grad_fn=<NllLossBackward0>)}\n",
            "Time statistics: Ave. batch time: 0.0\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-7ef4b437ed9c>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen_large_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m       \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m       \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "#@title Training the model\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "# meta training loop\n",
        "for meta_i in range(15):\n",
        "  #@title Build the model\n",
        "  nets = {str(n_layers): Net(input_size, optimized_ranks[str(n_layers)], n_layers , num_classes, with_skip=True) for n_layers in model_layer_configs}\n",
        "  for n_layers, net in nets.items():\n",
        "    print(f\"n_layers:{n_layers}, n_neurons: {sum(optimized_ranks[n_layers])+10+input_size}\")\n",
        "    # net.cuda()\n",
        "\n",
        "  #@title Define loss-function & optimizer\n",
        "\n",
        "  for n_layers, net in nets.items():\n",
        "    decay_params = chain(*[net.fc_layers[i].parameters() for i in range(net.n_layers+1)])\n",
        "    optimizers[n_layers] = torch.optim.Adam([{'params':decay_params, \"weight_decay\":0}, {'params':net.fc_layers[-1].parameters()}], lr=lr )\n",
        "\n",
        "    lr_scheds[n_layers] = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizers[n_layers], threshold=1e-2, verbose=True)\n",
        "\n",
        "    history[str(n_layers)] = []\n",
        "  batch_time_sum = 0\n",
        "  batch_count = 0\n",
        "  metric_time_sum = 0\n",
        "  metric_count = 0\n",
        "  prev_acc = defaultdict(lambda:0)\n",
        "  for epoch in range(num_epochs):\n",
        "    batch_time = time.time()\n",
        "\n",
        "    for i ,(images,labels) in enumerate(train_gen_small_batch):\n",
        "      images = Variable(images.view(-1,28*28))#.cuda()\n",
        "      labels = Variable(labels)#.cuda()\n",
        "\n",
        "      loss = {}\n",
        "      for n_layers, opt, net in zip(optimizers.keys(), optimizers.values(), nets.values()):\n",
        "        opt.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss[str(n_layers)] = loss_function(outputs, labels)\n",
        "        loss[str(n_layers)].backward(retain_graph=True)\n",
        "        opt.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      if (i+1) % 500==0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_data)//small_batch_size}], Loss: {loss}')\n",
        "        print(f\"Time statistics: Ave. batch time: {batch_time_sum/(batch_count or 1)}\")\n",
        "\n",
        "    for i ,(images,labels) in enumerate(train_gen_large_batch):\n",
        "      images = Variable(images.view(-1,28*28)).cuda()\n",
        "      labels = Variable(labels).cuda()\n",
        "\n",
        "      loss = {}\n",
        "      for n_layers, opt, net in zip(optimizers.keys(), optimizers.values(), nets.values()):\n",
        "        opt.zero_grad()\n",
        "        outputs = net(images)\n",
        "        loss[str(n_layers)] = loss_function(outputs, labels)\n",
        "        loss[str(n_layers)].backward(retain_graph=True)\n",
        "        opt.step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      if (i+1) % 500==0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_data)//large_batch_size}], Loss: {loss}')\n",
        "        print(f\"Time statistics: Ave. batch time: {batch_time_sum/(batch_count or 1)}\")\n",
        "    for n_layers, lr_sched in lr_scheds.items():\n",
        "      lr_sched.step(loss[str(n_layers)])\n",
        "    batch_time_sum += (time.time()-batch_time)\n",
        "    batch_count += 1\n",
        "\n",
        "    if (epoch+1) % 3 == 0:\n",
        "      metric_time = time.time()\n",
        "      rank_fc={}\n",
        "      rank_fc_grad={}\n",
        "      norm_fc={}\n",
        "      for n_layers, net in nets.items():\n",
        "        rank_fc[str(n_layers)] = []\n",
        "        rank_fc_grad[str(n_layers)] = []\n",
        "        norm_fc[str(n_layers)] = []\n",
        "        for layer in net.fc_layers:\n",
        "          rank_fc[str(n_layers)].append(np.linalg.matrix_rank(layer.weight.detach().cpu().numpy()))\n",
        "          if layer.weight.grad is not None:\n",
        "            rank_fc_grad[str(n_layers)].append(np.linalg.matrix_rank(layer.weight.grad.detach().cpu().numpy()))\n",
        "          norm_fc[str(n_layers)].append(np.linalg.norm(layer.weight.detach().cpu().numpy(), ord=2))\n",
        "\n",
        "      correct = {}\n",
        "      total = {}\n",
        "\n",
        "      for n_layers, net in nets.items():\n",
        "        correct[str(n_layers)] = 0\n",
        "        total[str(n_layers)] = 0\n",
        "\n",
        "\n",
        "        for images,labels in test_gen_large_batch:\n",
        "          images = Variable(images.view(-1,28*28)).cuda()\n",
        "          labels = labels.cuda()\n",
        "\n",
        "          output = net(images)\n",
        "\n",
        "          _, predicted = torch.max(output,1)\n",
        "\n",
        "          correct[str(n_layers)] += (predicted == labels).sum()\n",
        "          total[str(n_layers)] += labels.size(0)\n",
        "\n",
        "      metric_time_sum += time.time()-metric_time\n",
        "      metric_count += 1\n",
        "      evolve = True\n",
        "      for n_layers in nets.keys():\n",
        "        history[str(n_layers)].append({\"loss\":loss[str(n_layers)].item(), \"acc\":correct[str(n_layers)]/total[str(n_layers)], \"rank_fc\":rank_fc[str(n_layers)],\"rank_fc_grad\":rank_fc_grad[str(n_layers)], \"norm_fc\":norm_fc[str(n_layers)]})\n",
        "\n",
        "        print(f\"{n_layers}\")\n",
        "        print(f\"Test Accuracy:{correct[str(n_layers)]/total[str(n_layers)]}\")\n",
        "        print(f\"Ranks:{rank_fc[str(n_layers)]}\")\n",
        "        print(f\"Grad Ranks:{rank_fc_grad[str(n_layers)]}\")\n",
        "        print(f\"Layer sizes: {optimized_ranks}\")\n",
        "        print(f\"Trainable parameters:{sum(p.numel() for p in nets[n_layers].parameters() if p.requires_grad)}\")\n",
        "        print(f\"{sum(rank_fc[n_layers])/(sum(optimized_ranks[n_layers])+10)}\")\n",
        "        print(f\"Time statistics: Ave. batch time: {batch_time_sum/batch_count}, ave. metric time: {metric_time_sum/metric_count}\")\n",
        "\n",
        "        if correct[str(n_layers)]/total[str(n_layers)] < 0.9 or (correct[str(n_layers)]/total[str(n_layers)]-prev_acc[str(n_layers)])>0.01:\n",
        "          evolve = False\n",
        "        prev_acc[str(n_layers)] = correct[str(n_layers)]/total[str(n_layers)]\n",
        "\n",
        "\n",
        "      for n_layers, layer_ranks in rank_fc.items():\n",
        "        optimized_ranks[n_layers] = layer_ranks[:-1]\n",
        "      print(optimized_ranks)\n",
        "      if evolve:\n",
        "        print(\"Evolving!\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTPvMW5jHB9X"
      },
      "outputs": [],
      "source": [
        "#@title Evaluating the accuracy of the model\n",
        "\n",
        "correct = {}\n",
        "total = {}\n",
        "\n",
        "for n_layers, net in nets.items():\n",
        "  correct[str(n_layers)] = 0\n",
        "  total[str(n_layers)] = 0\n",
        "\n",
        "\n",
        "  for images,labels in test_gen:\n",
        "    images = Variable(images.view(-1,28*28)).cuda()\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    output = net(images)\n",
        "\n",
        "    _, predicted = torch.max(output,1)\n",
        "\n",
        "    correct[str(n_layers)] += (predicted == labels).sum()\n",
        "    total[str(n_layers)] += labels.size(0)\n",
        "\n",
        "  print(f'Accuracy of the model{n_layers}: {(100*correct[str(n_layers)])/(total[str(n_layers)]+1)}')\n",
        "  print(f\"{[sum(x['rank_fc']) for x in history[str(n_layers)]]}\")\n",
        "  print(f\"{sum(optimized_ranks[str(net.n_layers)])+10+input_size}\")\n",
        "  print(f\"{[sum(x['rank_fc'])/(sum(optimized_ranks[str(net.n_layers)])+10+input_size) for x in history[str(n_layers)]]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5mhb_ZU707X"
      },
      "outputs": [],
      "source": [
        "for n_layers, net in nets.items():\n",
        "  plt.plot([x[\"loss\"] for x in history[str(n_layers)]], [x[\"acc\"].cpu() for x in history[str(n_layers)]], \"*\", label=\"loss\")\n",
        "  plt.title(f\"{n_layers} Layers\")\n",
        "  plt.xlabel(\"Loss\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.show()\n",
        "  for i in range(net.n_layers+1):\n",
        "    plt.plot([x[\"rank_fc\"][i].cpu() for x in history[str(n_layers)]], [x[\"acc\"].cpu() for x in history[str(n_layers)]], \"*\", label=f\"rank {i}\")\n",
        "  plt.title(f\"{n_layers} Layers\")\n",
        "  plt.xlabel(\"Rank\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  for i in range(net.n_layers+1):\n",
        "    plt.plot([x[\"norm_fc\"][i].cpu().detach().numpy() for x in history[str(n_layers)]], [x[\"acc\"].cpu() for x in history[str(n_layers)]], \"*\", label=f\"norm {i}\")\n",
        "  plt.title(f\"{n_layers} Layers\")\n",
        "  plt.xlabel(\"Norm\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  plt.plot([x[\"loss\"] for x in history[str(n_layers)]], \"*\", label=\"loss\")\n",
        "  plt.plot([x[\"acc\"].cpu() for x in history[str(n_layers)]], \"*\", label=\"loss\")\n",
        "\n",
        "  plt.title(f\"{n_layers} Layers\")\n",
        "  plt.ylabel(\"Loss and Accuracy\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.show()\n",
        "\n",
        "  for i in range(net.n_layers+1):\n",
        "    plt.plot([x[\"rank_fc\"][i].cpu() for x in history[str(n_layers)]], \"*\", label=f\"rank {i}\")\n",
        "    plt.plot([x[\"norm_fc\"][i].cpu().detach().numpy() for x in history[str(n_layers)]], \"*\", label=f\"norm {i}\")\n",
        "  plt.title(f\"{n_layers} Layers\")\n",
        "  plt.ylabel(\"Rank and Norm\")\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.legend()\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrK5d9dpF8Q7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
